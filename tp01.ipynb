{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import collections\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from joblib import Parallel, delayed\n",
    "import nibabel as nib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get familiar with MR images\n",
    "\n",
    "The images we propose to load are in nifti format. This format embeds two kinds of information: the image itself (store in the data array), and some complementary about its acquisition (store in the header).\n",
    "The images can be seen under three views: axial, coronal or sagittal.\n",
    "Note also that the images you will see in this session and the next one show lesions.\n",
    "If you want to know more about MR imaging basic principles, you may find useful information in <font color='blue'> [Pooley et. al, RSNA, 2005]</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join(os.path.dirname(os.getcwd()), \"DB1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\Documents\\enseignement\\epita2019\\IMED\\DB1\\1\\orig\\reg_T1.nii.gz\n"
     ]
    }
   ],
   "source": [
    "impath = os.path.join(datapath, \"1\", \"orig\", \"reg_T1.nii.gz\")\n",
    "print(impath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimension: (240, 240, 48)\n",
      "Voxel size (mm): (0.958333, 0.958333, 3.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe53477dd7c44596a92cc4af51cd6e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='mr_slice', max=48), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the MR volume\n",
    "im = nib.load(impath)\n",
    "im_arr = im.get_data()\n",
    "\n",
    "#Get the image dimension (voxel)\n",
    "print(\"Image dimension:\", im_arr.shape)\n",
    "\n",
    "#Get the voxel size (mm)\n",
    "print(\"Voxel size:\", im.header.get_zooms())\n",
    "sx, sy, sz = im_arr.shape\n",
    "\n",
    "def show_axial(im_arr, mr_slice):\n",
    "    \"\"\"Show an axial slice of a MR image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im_arr: 3D array \n",
    "        the MR image to show\n",
    "    mr_slice: int\n",
    "        a slice number\n",
    "    \"\"\"\n",
    "    plt.imshow(arr[:, :, mr_slice].T, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "   \n",
    "#Show the MR volume, slice by slice, in axial view\n",
    "interactive_plot = interactive(show_axial, im_arr=fixed(im_arr), mr_slice=(0, sz, 1))\n",
    "interactive_plot      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MR imaging offers multiple advantages:\n",
    "- no X-rays,\n",
    "- good contrast in soft tissues,\n",
    "- availability of different image types in one machine (multimodality)...\n",
    "\n",
    "For example, you are vizualizing a T1 weighted MR image, slice by slice. The darkest parts of the images represent water and the brightest fat. Here, contrast agent has been previously injected to the patient before his scan, more specifically Gadolinium, allowing to enhance the image, which will be particularly useful to better delineate tumors, as we will see later.\n",
    "Moreover, MR imaging is useful to make the distinction between the different parts of the brain. \n",
    "Let's now explore the brain !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50317b7c22f24140914492c5a3841cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='mr_slice', max=48), Dropdown(description='tissue_type',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the manual segmentation of multiple tissues for the current patient\n",
    "gtpath = os.path.join(datapath, \"1\", \"segm.nii.gz\")\n",
    "gt = nib.load(gtpath)\n",
    "gt_arr = gt.get_data().astype(int)\n",
    "\n",
    "#Create the tissue list\n",
    "tissue_types = [\"Background\", \"Cortical gray matter\", \"Basal ganglia\", \"White matter\",\n",
    "                \"White matter lesions\", \"Cerebrospinal fluid\", \n",
    "                \"Ventricles\", \"Cerebellum\", \"Brain stem\", \"Infarction\", \"Other\"]\n",
    "\n",
    "#Define the function to superimpose the MR image and regions of interest contours in axial view\n",
    "def show_manual_seg(im_arr, seg_arr, mr_slice, tissue_type, tissue_types):\n",
    "    \"\"\"Superimpose segmentation contours to a MR image on an axial slice.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im_arr: 3D array \n",
    "        the MR image to show\n",
    "    seg_arr: 3D array \n",
    "        the segmentation image \n",
    "    mr_slice: int\n",
    "        a slice number\n",
    "    tissue_type: str\n",
    "        the brain tissue type\n",
    "    tissues_type: list of str\n",
    "        the list of available brain tissues\n",
    "    \"\"\"\n",
    "    tissue_label = np.where(np.array(tissue_types) == tissue_type)[0][0]\n",
    "    tissue_arr = (seg_arr[:, :, mr_slice] == tissue_label).astype(int)\n",
    "    plt.imshow(im_arr[:, :, mr_slice].T, cmap=\"gray\")\n",
    "    if np.count_nonzero(tissue_arr) > 0:\n",
    "        plt.contour(tissue_arr.T, colors=\"r\", linewidths=1, levels=[0.5, 1])\n",
    "    plt.axis('off')\n",
    "    plt.suptitle(tissue_types[tissue_label], fontsize=15)\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    \n",
    "#Show the MR volume, slice by slice, in axial view\n",
    "interactive_plot = interactive(show_manual_seg, im_arr=fixed(im_arr), seg_arr=fixed(gt_arr), \n",
    "                               mr_slice=(0, sz, 1), tissue_type=tissue_types, tissue_types=fixed(tissue_types))\n",
    "interactive_plot      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The axial view is particularly interesting for praticians, as its allows vizualizing the relative symmetry of the brain. Moreover, a brain lesion presence may be highlighted by a region of abnormal signal compared to the symmetrical region with respect to the inter-hemispheric plane (named the contralateral).  \n",
    "Depending on the shape of the structure of interest to study, the other views may also be useful to show.\n",
    "\n",
    "<font color='red'>Show the image in sagittal view</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8ebf7189ac4c05b16a9019b31edc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=120, description='mr_slice', max=240), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sagittal(im_arr, mr_slice):\n",
    "    #...To complete...\n",
    "interactive_plot = interactive(show_sagittal, im_arr=fixed(im_arr), mr_slice=#...To complete...)\n",
    "interactive_plot   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Show the image in coronal view.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b06efb86484f40b8b521e561b1038d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=120, description='mr_slice', max=240), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_coronal(im_arr, mr_slice):\n",
    "    #...To complete...\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "\n",
    "interactive_plot = interactive(show_coronal, im_arr=fixed(im_arr), mr_slice=#...To complete...)\n",
    "interactive_plot   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxel size is important when dealing with MR images. We are now going to prove it with two examples...\n",
    "\n",
    "<font color='red'>Compute the ventricules volume, before and after downsampling (factor 2) the manual segmentation image. Comment. What should you do to make these volumes similar ?</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Compute the distance between the cerebellum to the ventricles, before and after downsampling (factor 2) the manual segmentation image. Comment. What should you do to make these distances similar ?</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensity normalization\n",
    "\n",
    "Normalize the intensity is a common preprocessing step in MR imaging as there is not standard scale. It is also a mandatory step to further extract texture parameters, or in supervized segmentation. Try some methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear image quantization with grey levels in [0, 1]\n",
    "norm_im_arr = im_arr / np.max(im_arr)\n",
    "\n",
    "#Affine image quantization on 255 grey levels\n",
    "norm_im_arr = 254 * (im_arr - np.min(im_arr)) / (np.min(im_arr) - np.max(im_arr))\n",
    "\n",
    "#Zero-mean and unit variance\n",
    "mu = np.mean(im_arr)\n",
    "std = np.std(im_arr)\n",
    "norm_im_arr = (im_arr - mu) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the MR image to have zero-mean and unit variance is commonly use in Deep learning segmentation methods (see for example the Deepmedic paper from <font color='blue'> [Kamnitsask et. al, MIA, 2016]</font>). The code above normalizes the intensities on the whole image, but you can restrict the normalization to a specific tissue mask.\n",
    "\n",
    "<font color='red'>Modify the code above to have zero-mean and unit variance within the brain mask, computed by excluding the background from the manual segmentation.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Normalize the T1 images co-registered on the FLAIR of subject 1 and 4 to have zero-mean and unit variance within their respective brain mask. Superimpose the obtained histograms. Are they strictly the same ? Compute a histogram matching algorithm so that the histogram of image 2 is the same as the one of image 1 (within the brain mask of image 1 to simplify).</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_matching(im_arr, ref_arr, mask_arr):\n",
    "\n",
    "    \"\"\"Match the histogram of an image with the one of a reference image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: 3D array \n",
    "        the image to match\n",
    "    ref_arr: 3D array\n",
    "        the reference image\n",
    "    mask_arr: 3D array\n",
    "        the mask image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    im_match_arr: 3D array\n",
    "        the matched image\n",
    "    \"\"\"\n",
    "    \n",
    "    #...To complete...\n",
    "\n",
    "    return im_match_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are general, but some are specific to MR imaging (see <font color='blue'> [Shinohara et al., Neuroimage, 2014]</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-registration\n",
    "\n",
    "### Inter-modality and intra-patient co-registration\n",
    "\n",
    "Images are acquired from the same modality, but from different patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complet the following code to perform an ICP algorithm to co-register the brain surfaces on T1 images. Define a score to quantitavely assess the quality of the registration.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 48) (0.958333, 0.958333, 3.0)\n",
      "(240, 240, 48) (0.958333, 0.958333, 3.0)\n",
      "99711.0\n",
      "99711.0\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "#We are going to perform a simple co-registration algorithm, supposing that that two images only differ from a translation.\n",
    "#The translation transformation has 3 parameters, so only one voxel matching is needed to find \n",
    "#the best transformation parameter.\n",
    "\n",
    "impath1 = os.path.join(datapath, \"1\", \"orig\", \"FLAIR.nii.gz\")\n",
    "im1 = nib.load(impath1)\n",
    "im1_arr = im1.get_data()\n",
    "print(im1.shape, im1.header.get_zooms())\n",
    "\n",
    "impath2 = os.path.join(datapath, \"5\", \"orig\", \"FLAIR.nii.gz\")\n",
    "im2 = nib.load(impath2)\n",
    "im2_arr = im2.get_data()\n",
    "print(im2.shape, im2.header.get_zooms())\n",
    "\n",
    "#Load the manual segmentations\n",
    "gtpath1 = os.path.join(datapath, \"1\", \"segm.nii.gz\")\n",
    "gt1 = nib.load(gtpath1)\n",
    "gt1_arr = gt1.get_data().astype(int)\n",
    "\n",
    "gtpath2 = os.path.join(datapath, \"4\", \"segm.nii.gz\")\n",
    "gt2 = nib.load(gtpath2)\n",
    "gt2_arr = gt2.get_data().astype(int)\n",
    "\n",
    "#Compute each brain mask\n",
    "brain1_arr = (gt1_arr > 0).astype(int)\n",
    "brain2_arr = (gt2_arr > 0).astype(int)\n",
    "\n",
    "#Compute each brain surface array\n",
    "bs1_arr = #...To complete ...\n",
    "bs2_arr = #...To complete ...\n",
    "\n",
    "#Get the center of mass of the brain surface of image 2\n",
    "#...To complete ...\n",
    "x2_0 = #...To complete ...\n",
    "y2_0 = #...To complete ...\n",
    "z2_0 = #...To complete ...\n",
    "\n",
    "previous_error = 0.5\n",
    "error = 10000\n",
    "reg_arr = np.copy(bs1_arr)\n",
    "\n",
    "while error != previous_error:\n",
    "    \n",
    "    previous_error = error\n",
    "    \n",
    "    #Get the center of mass of each brain surface\n",
    "    #...To complete ...\n",
    "    x1_0 = #...To complete ...\n",
    "    y1_0 = #...To complete ...\n",
    "    z1_0 = #...To complete ...\n",
    "\n",
    "    #Get the parameters of the translation from M1 to M2\n",
    "    tx = #...To complete ...\n",
    "    ty =#...To complete ...\n",
    "    tz = #...To complete ...\n",
    "\n",
    "    #Compute the translated image 1 \n",
    "    reg_arr = #...To complete ...\n",
    "\n",
    "    #Compute the mean square error between the translated image 1 and image 2\n",
    "    error = #...To complete ...\n",
    "    print(error)\n",
    "    \n",
    "#Quantitative assessment\n",
    "#...To complete ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-modality and inter-patient co-registration\n",
    "\n",
    "Images acquired on the same machine, during the same acquisition session, but from different modalities.\n",
    "\n",
    "A common strategy in co-registration consists in several step:\n",
    "- criterion definition,\n",
    "- transformation parameter initialization,\n",
    "- transformation parameters optimization to optimize the criterion between the fixed and co-registered images.\n",
    "\n",
    "To simplify, we suppose that the searched transformation is a translation.\n",
    "when dealing with images from different modalities, the mutual information is a relevant criterion. We will use the normalize version (NMI) between two images A and B:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\textrm{NMI}(A, B) = \\frac{H(A) + H(B)}{H(A, B)}\n",
    "\\end{equation*}\n",
    "with H the Shannon entropy (see <font color='blue'> [Pluim et al., TMI, 2003] </font> for a review on mutual information criteria in medical image registration).\n",
    "\n",
    "<font color='red'>Complet the following code to co-register T1 and FLAIR images of the same patient optimizating a mutual information criterion.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a score to compare the co-registered image to the fixed image (here normalized mutual information)\n",
    "def nmi(t, im_arr, ref_arr):\n",
    "    \"\"\"Normalized mutual information between an image and its translated version\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im_arr: 3D array \n",
    "        the MR image to register\n",
    "    ref_arr: 3D array\n",
    "        the reference image\n",
    "    t: list of int [tx, ty, tz]\n",
    "        the translation parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "        the Normalized mutual information score\n",
    "    \"\"\"\n",
    "        \n",
    "    return score\n",
    "\n",
    "#Optimizations parameters\n",
    "options = {\n",
    "    \"xtol\": 1e-2,\n",
    "    \"disp\": True,\n",
    "    \"maxiter\": 500}\n",
    "\n",
    "#Transformation initialization\n",
    "t0 = #...To complete ...\n",
    "\n",
    "#Optimization\n",
    "res = minimize(nmi, t0, method=\"nelder-mead\",\n",
    "               args=(im_arr, ref_arr, ), options=options)\n",
    "\n",
    "#Get the best transform\n",
    "t = res.t\n",
    "\n",
    "#Apply it the the reference image\n",
    "#...To complete ...\n",
    "\n",
    "#Show the result\n",
    "#... To complete ...\n",
    "\n",
    "#Assess quantitatively the quality of the segmentation\n",
    "#... To complete ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "The co-registration we implemented included only a translation transform to simplify the calculation. However, it is not that simple when dealing with medical imaging. More approprieted transformations includes:\n",
    "- rigid (rotation + translation),\n",
    "- affine (rotation + translation + scaling),\n",
    "- non linear co-registration ...\n",
    "\n",
    "Many Python librairies offer robust medical imaging pre-processing, including co-registration algorithms:\n",
    "- FSL,\n",
    "- FreeSurfer,\n",
    "- ANTs ...\n",
    "\n",
    "We propose here to try the ones from ITK, but you are welcome to test other librairies.\n",
    "\n",
    "<font color='red'> Test methods from Python librairies, intra-modality/inter-patient or inter-modality/inter-patient, rigid, affine or non-linear. Do you improve the co-registration performances ? You may also use the T1 standard image.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-modality and inter-patient co-registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'SimpleITK.SimpleITK.Image'>\n",
      "(48, 240, 240)\n",
      "(240, 240, 48)\n",
      "(240, 240, 48)\n",
      "(240, 240, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e027ae14fa44ffb81680cdcd24c89a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='mr_slice', max=48), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the FLAIR image\n",
    "flairpath = os.path.join(datapath, \"1\", \"orig\", \"FLAIR.nii.gz\")\n",
    "flair = nib.load(flairpath)\n",
    "flair_arr = flair.get_data()\n",
    "\n",
    "#Load the native T1Gd image\n",
    "t1path = os.path.join(datapath, \"1\", \"orig\", \"T1.nii.gz\")\n",
    "t1 = nib.load(t1path)\n",
    "t1_arr = t1.get_data()\n",
    "\n",
    "fixed_image=sitk.ReadImage(flairpath,sitk.sitkFloat32)\n",
    "moving_image=sitk.ReadImage(t1path,sitk.sitkFloat32)\n",
    "\n",
    "initial_transform=sitk.CenteredTransformInitializer(fixed_image, moving_image, sitk.Euler3DTransform(),\n",
    "                                                    sitk.CenteredTransformInitializerFilter.GEOMETRY)\n",
    "\n",
    "registration_method=sitk.ImageRegistrationMethod()\n",
    "# Similarity metric settings.\n",
    "registration_method.SetMetricAsMattesMutualInformation(numberOfHistogramBins=50)\n",
    "registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)\n",
    "registration_method.SetMetricSamplingPercentage(0.01)\n",
    "registration_method.SetInterpolator(sitk.sitkLinear)\n",
    "# Optimizer settings.\n",
    "registration_method.SetOptimizerAsGradientDescent(learningRate=1.0,numberOfIterations=100,\n",
    "                                                  convergenceMinimumValue=1e-6,convergenceWindowSize=10)\n",
    "#registration_method.\n",
    "#SetOptimizerScalesFromPhysicalShift()\n",
    "\n",
    "final_transform=registration_method.Execute(fixed_image,moving_image)\n",
    "\n",
    "moving_resampled=sitk.Resample(moving_image,fixed_image,final_transform,\n",
    "                               sitk.sitkLinear,0.0,moving_image.GetPixelID())\n",
    "\n",
    "print(type(moving_resampled))\n",
    "move_arr = sitk.GetArrayFromImage(moving_resampled)\n",
    "print(move_arr.shape)\n",
    "print(flair_arr.shape)\n",
    "\n",
    "reg_t1_arr = np.zeros((flair_arr.shape))\n",
    "for mr_slice in range(0, sz):\n",
    "    reg_t1_arr[:, :, mr_slice] = move_arr[mr_slice, :, :]\n",
    "\n",
    "def show_axial2(im_arr, mr_slice):\n",
    "    plt.imshow(im_arr[:, :, mr_slice], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "print(flair_arr.shape)\n",
    "print(reg_t1_arr.shape)\n",
    "#Show the MR volume, slice by slice, in axial view\n",
    "interactive_plot = interactive(show_axial2, im_arr=fixed(reg_t1_arr), mr_slice=(0, sz, 1))\n",
    "interactive_plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intra-modality and inter-patient co-registration#Load the FLAIR image\n",
    "flairpath = os.path.join(datapath, \"1\", \"orig\", \"FLAIR.nii.gz\")\n",
    "flair = nib.load(flairpath)\n",
    "flair_arr = flair.get_data()\n",
    "\n",
    "#Load the native T1Gd image\n",
    "t1path = os.path.join(datapath, \"1\", \"orig\", \"T1.nii.gz\")\n",
    "t1 = nib.load(t1path)\n",
    "t1_arr = t1.get_data()\n",
    "\n",
    "fixed_image=sitk.ReadImage(flairpath,sitk.sitkFloat32)\n",
    "moving_image=sitk.ReadImage(t1path,sitk.sitkFloat32)\n",
    "\n",
    "initial_transform=sitk.CenteredTransformInitializer(fixed_image, moving_image, sitk.Euler3DTransform(),\n",
    "                                                    sitk.CenteredTransformInitializerFilter.GEOMETRY)\n",
    "\n",
    "registration_method=sitk.ImageRegistrationMethod()\n",
    "# Similarity metric settings.\n",
    "registration_method.SetMetricAsMattesMutualInformation(numberOfHistogramBins=50)\n",
    "registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)\n",
    "registration_method.SetMetricSamplingPercentage(0.01)\n",
    "registration_method.SetInterpolator(sitk.sitkLinear)\n",
    "# Optimizer settings.\n",
    "registration_method.SetOptimizerAsGradientDescent(learningRate=1.0,numberOfIterations=100,\n",
    "                                                  convergenceMinimumValue=1e-6,convergenceWindowSize=10)\n",
    "#registration_method.\n",
    "#SetOptimizerScalesFromPhysicalShift()\n",
    "\n",
    "final_transform=registration_method.Execute(fixed_image,moving_image)\n",
    "\n",
    "moving_resampled=sitk.Resample(moving_image,fixed_image,final_transform,\n",
    "                               sitk.sitkLinear,0.0,moving_image.GetPixelID())\n",
    "\n",
    "print(type(moving_resampled))\n",
    "move_arr = sitk.GetArrayFromImage(moving_resampled)\n",
    "print(move_arr.shape)\n",
    "print(flair_arr.shape)\n",
    "\n",
    "reg_t1_arr = np.zeros((flair_arr.shape))\n",
    "for mr_slice in range(0, sz):\n",
    "    reg_t1_arr[:, :, mr_slice] = move_arr[mr_slice, :, :]\n",
    "\n",
    "def show_axial2(im_arr, mr_slice):\n",
    "    plt.imshow(im_arr[:, :, mr_slice], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "print(flair_arr.shape)\n",
    "print(reg_t1_arr.shape)\n",
    "#Show the MR volume, slice by slice, in axial view\n",
    "interactive_plot = interactive(show_axial2, im_arr=fixed(reg_t1_arr), mr_slice=(0, sz, 1))\n",
    "interactive_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhomogeneity correction\n",
    "\n",
    "You may try the N4BiasFieldCorrection function to perform bias correction on MR images. Compare the obtained images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
